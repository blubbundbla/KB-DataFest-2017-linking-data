{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration of Linking Phenoscape and iDigBio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lower, split, sum\n",
    "from pyspark.sql.types import IntegerType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linking based on specimen identifiers in Phenoscape\n",
    "\n",
    "Phenoscape's first project included the identifiers of specimens taken from publications with the phenotypes collected. Some limitations:\n",
    "\n",
    "1. Only the catalog number and institution code are entered\n",
    "1. The specimens identified were mentioned in the paper with the phenotypes, there is not a 1:1 correspondence between the measured character and the specific specimen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading data from Phenoscape\n",
    "\n",
    "The SPARQL interface (http://yasgui.org/ with the endpoint http://db.phenoscape.org/bigsparql) was used to download a CSV with all distinct specimen identifiers. The query was provided by Jim Balhof:\n",
    "\n",
    "```\n",
    "PREFIX obo: <http://purl.obolibrary.org/obo/>\n",
    "PREFIX ps: <http://purl.org/phenoscape/vocab.owl#>\n",
    "PREFIX rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#> \n",
    "PREFIX rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "PREFIX owl: <http://www.w3.org/2002/07/owl#>\n",
    "PREFIX BFO: <http://purl.obolibrary.org/obo/BFO_>\n",
    "PREFIX fin: <http://purl.obolibrary.org/obo/UBERON_0008897>\n",
    "PREFIX dc: <http://purl.org/dc/elements/1.1/>\n",
    "PREFIX hint: <http://www.bigdata.com/queryHints#> \n",
    "PREFIX pedal_digit_1: <http://purl.obolibrary.org/obo/UBERON_0003631>\n",
    "PREFIX pectoral_fin: <http://purl.obolibrary.org/obo/UBERON_0000151>\n",
    "PREFIX prehallux: <http://purl.obolibrary.org/obo/UBERON_0012136>\n",
    "PREFIX Anura: <http://purl.obolibrary.org/obo/VTO_0002210>\n",
    "PREFIX length: <http://purl.obolibrary.org/obo/PATO_0000122>\n",
    "PREFIX dwc: <http://rs.tdwg.org/dwc/terms/>\n",
    "PREFIX has_external_reference: <http://purl.obolibrary.org/obo/CDAO_0000164>\n",
    "SELECT DISTINCT ?matrix_taxon ?vto ?vto_label ?collection ?catNum WHERE {\n",
    "?otu dwc:individualID ?specimen .\n",
    "?otu rdfs:label ?matrix_taxon .\n",
    "?otu has_external_reference: ?vto .\n",
    "?vto rdfs:label ?vto_label .\n",
    "?specimen dwc:collectionID/rdfs:label ?collection .\n",
    "?specimen dwc:catalogNumber ?catNum .\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The downloaded CSV had spaces around the delimiter that needed to be trimmed away to have it import nicely into Spark. See `phenoscape_dl_fixup.sh`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characterizing the downloaded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load CSV after placing it on HDFS\n",
    "ps_spec = (sqlContext\n",
    "           .read\n",
    "           .option(\"header\", \"true\")\n",
    "           .csv(\"/home/mjcollin/queryResults_phenoscape_specimens.csv\")\n",
    "           .cache()\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12283\n",
      "root\n",
      " |-- matrix_taxon: string (nullable = true)\n",
      " |-- vto: string (nullable = true)\n",
      " |-- vto_label: string (nullable = true)\n",
      " |-- collection: string (nullable = true)\n",
      " |-- catNum: string (nullable = true)\n",
      "\n",
      "+--------------------+--------------------+--------------------+----------+------+\n",
      "|        matrix_taxon|                 vto|           vto_label|collection|catNum|\n",
      "+--------------------+--------------------+--------------------+----------+------+\n",
      "|Scyliorhinus retifer|http://purl.oboli...|Scyliorhinus retifer|      AMNH| 36777|\n",
      "|     Mustelus laevis|http://purl.oboli...|   Mustelus mustelus|      AMNH|  4138|\n",
      "|   Channallabes apus|http://purl.oboli...|   Channallabes apus|      AMNH|  6631|\n",
      "+--------------------+--------------------+--------------------+----------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ps_spec.count())\n",
    "ps_spec.printSchema()\n",
    "ps_spec.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets see what the quality of the catalog numbers are by looking for things that have remarks, text, and other non-catalog number type information stored in the field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115\n",
      "+-------------------------------------------------------------------+-----+\n",
      "|catNum                                                             |count|\n",
      "+-------------------------------------------------------------------+-----+\n",
      "|uncat.                                                             |210  |\n",
      "|V-s/n                                                              |36   |\n",
      "|uncatalogued                                                       |23   |\n",
      "|não catalogado                                                     |20   |\n",
      "|não catalog.                                                       |10   |\n",
      "|Uncat.                                                             |7    |\n",
      "|F uncat.                                                           |5    |\n",
      "|nåo catalog.                                                       |4    |\n",
      "|não catalog                                                        |4    |\n",
      "|uncat                                                              |4    |\n",
      "|NRA                                                                |4    |\n",
      "|HAK                                                                |3    |\n",
      "|uncataloged                                                        |3    |\n",
      "|nåo catalog                                                        |3    |\n",
      "|no catalog number (2 specimens)                                    |2    |\n",
      "|unregistered                                                       |2    |\n",
      "|64-229-42A (specimen dissected)                                    |1    |\n",
      "|no number (skeleton)                                               |1    |\n",
      "|V73:11710 (dissected)                                              |1    |\n",
      "|HDJ                                                                |1    |\n",
      "|1654CS (dissected)                                                 |1    |\n",
      "|IG 23151-152 (part and counter-part, ex NS1-2) etc.                |1    |\n",
      "|1697CS (dissected)                                                 |1    |\n",
      "|31170 [1906-9-6: 7], X-rays 291, two individuals  (Lake Tanganyika)|1    |\n",
      "|P 36203 (ex †L. gibbus)                                            |1    |\n",
      "+-------------------------------------------------------------------+-----+\n",
      "only showing top 25 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "poor_cat_nums = (ps_spec\n",
    "                 .filter(~ col(\"catNum\").rlike(\"[0-9]\")\n",
    "                        | col(\"catNum\").like(\"%(%\"))\n",
    "                 .groupBy(col(\"catNum\"))\n",
    "                 .count()\n",
    "                 .orderBy(col(\"count\"), ascending=False)\n",
    "                 )\n",
    "\n",
    "print(poor_cat_nums.count())\n",
    "poor_cat_nums.show(25, truncate=False)\n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+\n",
      "|sum(CAST(count AS INT))|\n",
      "+-----------------------+\n",
      "|                    439|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(poor_cat_nums\n",
    " .select(sum(col(\"count\").cast(IntegerType())))\n",
    " .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It looks like we should set aside 439 out of the 12283 specimen ids as being not valid or requireing interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What collections are represented?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "+----------+-----+\n",
      "|collection|count|\n",
      "+----------+-----+\n",
      "|      USNM| 1584|\n",
      "|      AMNH| 1116|\n",
      "|     MZUSP|  950|\n",
      "|      UMMZ|  834|\n",
      "|      BMNH|  626|\n",
      "|      FMNH|  583|\n",
      "|       UAM|  548|\n",
      "|        KU|  546|\n",
      "|       OSM|  491|\n",
      "|      HUMZ|  396|\n",
      "+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ps_collections = (ps_spec\n",
    "                  .groupBy(col(\"collection\"))\n",
    "                  .count()\n",
    "                  .orderBy(col(\"count\"), ascending=False)\n",
    ")\n",
    "print(ps_collections.count())\n",
    "ps_collections.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which of these are in iDigBio? Here's where we'll need to load up iDigBio to take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "idb_df = sqlContext.read.parquet(\"/guoda/data/idigbio-20171209T023310.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106202428\n"
     ]
    }
   ],
   "source": [
    "print(idb_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['usnm', 'amnh', 'mzusp', 'ummz', 'bmnh', 'fmnh', 'uam', 'ku', 'osm', 'humz']\n"
     ]
    }
   ],
   "source": [
    "ps_collections_list = [i.collection.lower() for i in ps_collections.collect()]\n",
    "print(ps_collections_list[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n",
      "+---------------+-------+\n",
      "|institutioncode|  count|\n",
      "+---------------+-------+\n",
      "|           mnhn|7116859|\n",
      "|           usnm|5020984|\n",
      "|             ku|2690964|\n",
      "|            cas|2417139|\n",
      "|           fmnh|1854607|\n",
      "|            mcz|1844567|\n",
      "|             uf|1699668|\n",
      "|            ypm|1395137|\n",
      "|           amnh|1140963|\n",
      "|             cm| 981091|\n",
      "+---------------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idb_institutions = (idb_df\n",
    "                   .filter(col(\"institutioncode\").isin(ps_collections_list))\n",
    "                   .groupBy(col(\"institutioncode\"))\n",
    "                   .count()\n",
    "                   .orderBy(col(\"count\"), ascending=False)\n",
    "                   )\n",
    "print(idb_institutions.count())\n",
    "idb_institutions.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 69 of the 128 collection codes are not in the institution field. Are they in the collection code field?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49\n",
      "+--------------+------+\n",
      "|collectioncode| count|\n",
      "+--------------+------+\n",
      "|            pc|461954|\n",
      "|            uf|347466|\n",
      "|          uaic|188772|\n",
      "|          kuvp| 82676|\n",
      "|            iu| 71561|\n",
      "|          uamz| 33936|\n",
      "|            os|  5114|\n",
      "|            ku|  2161|\n",
      "|          lacm|  2152|\n",
      "|          tcwc|  2024|\n",
      "+--------------+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idb_collections = (idb_df\n",
    "                   .filter(col(\"collectioncode\").isin(ps_collections_list))\n",
    "                   .groupBy(col(\"collectioncode\"))\n",
    "                   .count()\n",
    "                   .orderBy(col(\"count\"), ascending=False)\n",
    "                   )\n",
    "print(idb_collections.count())\n",
    "idb_collections.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup, a bunch match here too. We'll have to be more generous in the interpretation of the field which will likely lead to over/false matches, especially since there look to be a number of 2-letter codes which are unlikely to be unambigious.\n",
    "\n",
    "Perhaps there are collections are not in iDigBio? How many specimens from Phenoscape don't have a collection code in iDigBio in either the institution or collection field?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'osm', 'ihb', 'saiab', 'scnu', 'frsku', 'fml', 'icnmhn', 'zsi', 'arc', 'mhnls', 'bsku', 'mbucv', 'nwafc', 'ams', 'cmk', 'mapa', 'gvf', 'faku', 'mac-pay', 'mzb', 'furg', 'zumt', 'ntm', 'msnvr', 'ncip', 'mcng', 'amg', 'nhrm', 'vims', 'cas-iu', 'mzuabcs', 'mepn', 'kfrs', 'izua', 'su', 'fsfrl', 'kiz', 'humz', 'fau', 'huj', 'ibrp', 'ummp', 'adr', 'mcp', 'nlu', 'lgp', 'mm', 'cas-su', 'smwu', 'dbav.uerj', 'mhnm', 'kumf', 'lbuch'}\n"
     ]
    }
   ],
   "source": [
    "idb_institution_set = set([i.institutioncode for i in idb_institutions.collect()])\n",
    "idb_collection_set = set([i.collectioncode for i in idb_collections.collect()])\n",
    "missing_collections = (set(ps_collections_list) - idb_institution_set) - idb_collection_set\n",
    "print(missing_collections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53\n",
      "+----------+-----+\n",
      "|collection|count|\n",
      "+----------+-----+\n",
      "|       OSM|  491|\n",
      "|      HUMZ|  396|\n",
      "|       MCP|  251|\n",
      "|     MBUCV|  190|\n",
      "|       AMS|  102|\n",
      "|    CAS-SU|   87|\n",
      "|     MSNVR|   63|\n",
      "|        SU|   62|\n",
      "|       KIZ|   54|\n",
      "|      MCNG|   49|\n",
      "+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "+----------+\n",
      "|sum(count)|\n",
      "+----------+\n",
      "|      2038|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "missing_coll_counts = (ps_collections\n",
    "                       .filter(lower(col(\"collection\")).isin(missing_collections))\n",
    "                       .orderBy(col(\"count\"), ascending=False)\n",
    "                       )\n",
    "print(missing_coll_counts.count())\n",
    "missing_coll_counts.show(10)\n",
    "(missing_coll_counts\n",
    " .select(sum(col(\"count\")))\n",
    " .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22699\n",
      "+--------------------+----------+------+--------------------+\n",
      "|        matrix_taxon|collection|catNum|      scientificname|\n",
      "+--------------------+----------+------+--------------------+\n",
      "|Opisthopterus tar...|      USNM|283232|  bucephala clangula|\n",
      "|    Etrumeus sardina|      USNM|188950|     etrumeus sadina|\n",
      "|      Etrumeus teres|      USNM|188950|     etrumeus sadina|\n",
      "|Pogonopoma werthe...|      USNM|302292|                null|\n",
      "|     Brycinus brevis|      USNM|179332|     cancer borealis|\n",
      "|Maculirhamdia sti...|      USNM|326389|myrmeciza atrotho...|\n",
      "|Acrochordonichthy...|      FMNH| 68010|lasmigona costata...|\n",
      "|  Galaxias fasciatus|      USNM|203882|vulpes vulpes ala...|\n",
      "|     Notropis procne|        KU| 18877|      plesiosminthus|\n",
      "|Glandulocauda mel...|      FMNH| 15026|lampsilis teres (...|\n",
      "+--------------------+----------+------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ps_idb = (ps_spec\n",
    "         .join(idb_df, \n",
    "               ((lower(col(\"collection\")) == col(\"collectioncode\")) \n",
    "                 | (lower(col(\"collection\")) == col(\"institutioncode\")))\n",
    "               & (lower(col(\"catNum\")) == col(\"catalognumber\"))\n",
    "              )\n",
    "          )\n",
    "print(ps_idb.count())\n",
    "ps_idb.select(col(\"matrix_taxon\"), col(\"collection\"), col(\"catNum\"), col(\"scientificname\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It certainly looks like a bunch of the taxonomy does not match and there seem to be about double the number of expected records. There are likely multiple code/number pairs matching. A manual example made by looking in the iDigBio portal for institution code = \"ku\" and catalog number = \"15595\" yeilds 7 records found. Two examples:\n",
    "\n",
    "http://portal.idigbio.org/portal/records/ed17b627-0d7d-4996-a9f2-6ae9bdb74ebb\n",
    "\n",
    "http://portal.idigbio.org/portal/records/9c7f810f-6f42-403a-8adc-0d3b4866141e\n",
    "\n",
    "Ignoring taxonomy, how much duplication is there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+-----+\n",
      "|collection|catNum|count|\n",
      "+----------+------+-----+\n",
      "|       MCZ|  7714|   25|\n",
      "|      USNM|130183|   24|\n",
      "|      USNM|288474|   18|\n",
      "|      FMNH|  6724|   16|\n",
      "|        KU| 18440|   16|\n",
      "|        KU| 12442|   16|\n",
      "|      FMNH| 10489|   16|\n",
      "|      USNM|218830|   15|\n",
      "|       CAS| 55554|   15|\n",
      "|      USNM| 39529|   14|\n",
      "|      USNM|232930|   14|\n",
      "|      USNM|278989|   14|\n",
      "|      FMNH|  7592|   14|\n",
      "|      FMNH| 10064|   14|\n",
      "|      USNM|220851|   12|\n",
      "|      USNM|236399|   12|\n",
      "|      USNM|273737|   12|\n",
      "|      USNM|188950|   12|\n",
      "|      USNM|229884|   12|\n",
      "|      USNM|175436|   12|\n",
      "+----------+------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(ps_idb\n",
    ".groupBy(col(\"collection\"), col(\"catNum\"))\n",
    ".count()\n",
    ".orderBy(col(\"count\"), ascending=False)\n",
    ".show(20)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Certainly a lot. What to add for taxonomy? Scientific name (or the concatenation of genus and specific epithet) in iDigBio and the matrix_taxon column have some variation with sp. and author junk. Would just matching the genus be good enough? Presumably that would disambiguate the multiple collections at an institution and address the apparent conflation of the instution code and collection code concepts that appear to be present in Phenoscape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2936\n",
      "+--------------------+----------+------+--------------------+\n",
      "|        matrix_taxon|collection|catNum|      scientificname|\n",
      "+--------------------+----------+------+--------------------+\n",
      "|    Etrumeus sardina|      USNM|188950|     etrumeus sadina|\n",
      "|      Etrumeus teres|      USNM|188950|     etrumeus sadina|\n",
      "|   Erimyzon succetta|      USNM|129386|    erimyzon sucetta|\n",
      "|Argopleura magdal...|      USNM|235922|argopleura magdal...|\n",
      "|      Argopleura sp.|      USNM|235922|argopleura magdal...|\n",
      "|    Epapterus blohmi|      USNM|257983|    epapterus blohmi|\n",
      "|Bunocephalus cora...|      USNM|284646|bunocephalus cora...|\n",
      "|  Brycinus lateralis|      USNM|310101|  brycinus lateralis|\n",
      "|  Eugnathichthys sp.|      USNM|326204|      eugnathichthys|\n",
      "|Bathytyphlops mar...|      USNM|341861|bathytyphlops mar...|\n",
      "+--------------------+----------+------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ps_idb_inc_tax = (ps_idb\n",
    "                  .filter(lower(split(col(\"matrix_taxon\"),\" \")[0]) == col(\"genus\"))\n",
    "                  )\n",
    "print(ps_idb_inc_tax.count())\n",
    "ps_idb_inc_tax.select(col(\"matrix_taxon\"), col(\"collection\"), col(\"catNum\"), col(\"scientificname\")).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----------+------+-----+\n",
      "|            genus|collection|catNum|count|\n",
      "+-----------------+----------+------+-----+\n",
      "|     glyptothorax|      USNM|288474|    3|\n",
      "|      astroblepus|      FMNH| 96628|    2|\n",
      "|   breitensteinia|      USNM|230304|    2|\n",
      "|     bunocephalus|        UF| 33074|    2|\n",
      "|         etrumeus|      USNM|188950|    2|\n",
      "|       citharinus|      USNM|231554|    2|\n",
      "|          hoplias|      USNM|226265|    2|\n",
      "|   aphyocheirodon|      FMNH| 57821|    2|\n",
      "|      apteronotus|      FMNH|  7592|    2|\n",
      "|      eigenmannia|      USNM|260240|    2|\n",
      "|     bunocephalus|       MCZ| 48566|    2|\n",
      "|     bunocephalus|       MCZ| 46133|    2|\n",
      "|       argopleura|      ANSP|127516|    2|\n",
      "|         brycinus|      USNM|310088|    2|\n",
      "|      eigenmannia|      USNM|260242|    2|\n",
      "|     bunocephalus|       ROM| 62223|    2|\n",
      "|     bunocephalus|        UF| 77836|    2|\n",
      "|           bagrus|      USNM|229884|    2|\n",
      "|     bunocephalus|      USNM|121244|    2|\n",
      "|      hypoptopoma|      FMNH| 85814|    2|\n",
      "|      apteronotus|      FMNH| 56775|    2|\n",
      "|     hysteronotus|      USNM|331834|    2|\n",
      "|      astroblepus|       MCZ| 31512|    2|\n",
      "|             esox|      FMNH|  6724|    2|\n",
      "|             esox|      FMNH| 10489|    2|\n",
      "|      hypoptopoma|      FMNH| 85826|    2|\n",
      "|    euchiloglanis|      USNM|120365|    2|\n",
      "|          hoplias|      USNM|308914|    2|\n",
      "|           mystus|       CAS| 55554|    2|\n",
      "|      distichodus|      USNM|175436|    2|\n",
      "|       planaltina|      USNM|278989|    2|\n",
      "|          synodus|      USNM|315318|    2|\n",
      "|      sternopygus|      USNM|218830|    2|\n",
      "|     glyptothorax|       MCZ| 47232|    2|\n",
      "|     bunocephalus|      USNM|260191|    2|\n",
      "|  pseudorinelepis|      FMNH| 95570|    2|\n",
      "|         notropis|        KU|  3084|    2|\n",
      "|       argopleura|      USNM|235922|    2|\n",
      "|             esox|      FMNH| 31768|    2|\n",
      "|      bagrichthys|       CAS| 60713|    2|\n",
      "|   heteropneustes|      USNM|273737|    2|\n",
      "|     hoplosternum|      FMNH| 84055|    1|\n",
      "|         notropis|        KU|  8364|    1|\n",
      "|         hemiodus|       MCZ| 52668|    1|\n",
      "|        oxyropsis|      ANSP|138871|    1|\n",
      "|microlepidogaster|      USNM|300909|    1|\n",
      "|         enchodus|      KUVP|   821|    1|\n",
      "|         notropis|        KU|  4198|    1|\n",
      "|         hybopsis|        KU| 18885|    1|\n",
      "|         enchodus|      KUVP| 66148|    1|\n",
      "+-----------------+----------+------+-----+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(ps_idb_inc_tax\n",
    ".groupBy(col(\"genus\"), col(\"collection\"), col(\"catNum\"))\n",
    ".count()\n",
    ".orderBy(col(\"count\"), ascending=False)\n",
    ".show(50)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not perfect but better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write out the result to fiddle with more later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o893.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:215)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:173)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:173)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:173)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:145)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:438)\n\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:474)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:610)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:233)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:217)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:509)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 35 in stage 115.0 failed 4 times, most recent failure: Lost task 35.3 in stage 115.0 (TID 10295, mesos12.acis.ufl.edu, executor 2): java.lang.OutOfMemoryError: GC overhead limit exceeded\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:188)\n\t... 45 more\nCaused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-1001d2cb66e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mps_idb_inc_tax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'overwrite'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/tmp/ps_idb_inc_tax.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/latest/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mparquet\u001b[0;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[1;32m    689\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartitionBy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_opts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 691\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/latest/python/lib/py4j-latest-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1133\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/latest/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/latest/python/lib/py4j-latest-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    318\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    320\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o893.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:215)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:173)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:173)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:173)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:145)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:438)\n\tat org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:474)\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:117)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:138)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:135)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:116)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:92)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:610)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:233)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:217)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:509)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:280)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 35 in stage 115.0 failed 4 times, most recent failure: Lost task 35.3 in stage 115.0 (TID 10295, mesos12.acis.ufl.edu, executor 2): java.lang.OutOfMemoryError: GC overhead limit exceeded\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:188)\n\t... 45 more\nCaused by: java.lang.OutOfMemoryError: GC overhead limit exceeded\n"
     ]
    }
   ],
   "source": [
    "ps_idb_inc_tax.write.mode('overwrite').parquet(\"/tmp/ps_idb_inc_tax.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions of Linking Using Specimen Identifiers\n",
    "\n",
    "About 2900 specimen identifiers out of about 12000 valid ones (24%) were able to be linked to a single specimen in iDigBio using the catalog number, the genus, and by matching the collection code to either the institution or collection codes in iDigBio. There were 439 invalid catalog numbers that contained something other than a bare catalog number in Phenoscape. In addition, there were about 40 combinations of those three fields that occured more than once in iDigBio.\n",
    "\n",
    "This match rate is pretty low. With some inspection there some international collections that are missing from iDigBio (\"UFRJ\" or Federal University of Rio de Janeiro, \"HUMZ\" or Hokkaido University, Laboratory of Marine Zoology, etc.) that account for 2038 unjoined records. An addtional (12283 - 2900 - 439 - 2038) / (12283) = 56% were unable to be matched to a single record with no explaination.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3 - PySpark Python3 Medium",
   "language": "python",
   "name": "pyspark3-med"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
